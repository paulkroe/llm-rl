{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-17 23:07:01,664] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:07:05 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 09-17 23:07:05 [__init__.py:239] Automatically detected platform cuda.\n",
      "Logs and Checkpoints will be saved to: /root/workspace/scratch/deepseek_r1z_hackathon/r1-zero\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import time\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import socket\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from deepspeed import DeepSpeedEngine\n",
    "import deepspeed\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.inputs import TokensPrompt\n",
    "\n",
    "import wandb\n",
    "\n",
    "def find_free_port():\n",
    "    \"\"\"Find a free port on localhost.\"\"\"\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"\", 0))\n",
    "        s.listen(1)\n",
    "        port = s.getsockname()[1]\n",
    "    return port\n",
    "\n",
    "# Needed to stop DeepSpeed from complaining\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "SCRATCH = Path.home() / \"workspace\" / \"scratch\"\n",
    "os.environ[\"HF_HOME\"] = str(SCRATCH / \"hf_home\")\n",
    "\n",
    "RUN_NAME = \"r1-zero\"\n",
    "EXP_DIR = SCRATCH / \"deepseek_r1z_hackathon\" / RUN_NAME\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Logs and Checkpoints will be saved to: {EXP_DIR}\")\n",
    "\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "DATASET_NAME = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-1.5B\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "EOS_TOKEN = \"<|endoftext|>\" # has to match the model\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant. You first think about the reasoning process \"\n",
    "    \"and then provide the user with the answer.\"\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Using the numbers {numbers}, create an equation that equals {target}. \"\n",
    "    \"You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. \"\n",
    "    \"Show your work in <think> </think> tags. And return the final equation and answer in \"\n",
    "    \"<answer> </answer> tags, for example <answer>(1 + 2) / (3 * 5)</answer>.\"\n",
    ")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HbtzsG-yL6pd"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Tuple\n",
    "def format_reward_func(completion: str) -> float:\n",
    "    \"\"\"\n",
    "    Format: <think>...</think>\\n</answer>...</answer>\n",
    "\n",
    "    Also checks that the content within <answer>...</answer> conforms to a\n",
    "    specified pattern (only digits, + - * / ( ) . and whitespace).\n",
    "\n",
    "    Args:\n",
    "        completion (str): Generated output\n",
    "\n",
    "    Returns:\n",
    "        float: Reward score\n",
    "    \"\"\"\n",
    "    # Define the allowed pattern (only numbers, +, -, *, /, (, ), ., and whitespace)\n",
    "    allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "\n",
    "    try:\n",
    "        # add synthetic <think> as its already part of the prompt and prefilled\n",
    "        # for the assistant to more easily match the regex\n",
    "        completion = \"<think>\" + completion\n",
    "\n",
    "        # Strip EOS token if present\n",
    "        if completion.endswith(EOS_TOKEN):\n",
    "            completion = completion[:-len(EOS_TOKEN)]\n",
    "\n",
    "        # Check if the format is correct\n",
    "        # Pattern means:\n",
    "        # 1) <think>...contents not including other <think> tags...</think>\n",
    "        # 2) \\n\n",
    "        # 3) <answer>...anything...</answer>\n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            # Format is incorrect\n",
    "            return 0.0\n",
    "        else:\n",
    "            # Extract the content inside <answer>...</answer>\n",
    "            answer_content = match.group(2).strip()\n",
    "\n",
    "            # Check if answer content matches the allowed pattern\n",
    "            if not re.match(allowed_pattern, answer_content):\n",
    "                # If it doesn't match, reward is 0.5\n",
    "                return 0.5\n",
    "            else:\n",
    "                # If both format and pattern are correct, reward is 1\n",
    "                return 1.0\n",
    "    except Exception:\n",
    "        # Any error leads to 0 reward\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def equation_reward_func(completion: str, nums: List[int], target: int) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates completion based on mathematical correctness of the answer\n",
    "\n",
    "    Args:\n",
    "        completion (str): Generated output\n",
    "        target (str): Expected answer\n",
    "        nums (list): Available numbers to use in the equation\n",
    "\n",
    "    Returns:\n",
    "        float: Reward score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the format is correct\n",
    "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
    "        if match is None:\n",
    "            return 0.0\n",
    "        # Extract the \"answer\" part from the completion\n",
    "        equation = match.group(1).strip()\n",
    "        # Extract all numbers from the equation\n",
    "        used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n",
    "\n",
    "        # Check if all numbers are used exactly once\n",
    "        if sorted(used_numbers) != sorted(nums):\n",
    "            return 0.0\n",
    "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
    "        allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "        if not re.match(allowed_pattern, equation):\n",
    "            return 0.0\n",
    "\n",
    "        # Evaluate the equation with restricted globals and locals\n",
    "        result = eval(equation, {\"__builtins__\": None}, {})\n",
    "        # Check if the equation is correct and matches the ground truth\n",
    "        if abs(float(result) - float(target)) < 1e-5:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    except Exception:\n",
    "        # If evaluation fails, reward is 0\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def compute_reward(completion: str, sample: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:\n",
    "    nums = sample[\"nums\"]\n",
    "    target = sample[\"target\"]\n",
    "\n",
    "    format_reward = format_reward_func(completion)\n",
    "    equation_reward = equation_reward_func(\n",
    "        completion=completion, nums=nums, target=target\n",
    "    )\n",
    "\n",
    "    reward = format_reward + equation_reward\n",
    "\n",
    "    metrics = {\n",
    "        \"format_reward\": format_reward,\n",
    "        \"equation_reward\": equation_reward,\n",
    "    }\n",
    "\n",
    "    return reward, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0yQ1AkuILdoY"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def process_sample(example: Dict[str, int]) -> Dict[str, int]:\n",
    "    prefix = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(numbers=example[\"nums\"], target=example[\"target\"])},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let me reason through this step by step.\\n<think>\"},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prefix, tokenize=True, continue_final_message=True\n",
    "    )\n",
    "    prompt = tokenizer.decode(\n",
    "        input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"prompt_token_ids\": input_ids}\n",
    "\n",
    "dataset = dataset.map(process_sample, num_proc=6)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4asCd4p0TRYN"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentConfig():\n",
    "    model_name: str = MODEL_NAME\n",
    "\n",
    "    # hyperparameters for sampling\n",
    "    gen_per_sample: int = 4\n",
    "    temperature: float = 1.0\n",
    "    max_new_tokens: int = 1024\n",
    "    top_p: float = 1.0  # disable top_p\n",
    "    top_k: int = -1     # disable tok_k\n",
    "    num_rollouts: int = 16\n",
    "    kl_coefficient: float = 0.001\n",
    "    n_rollouts: int = 64\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 1e-6\n",
    "\n",
    "# Now you can use the model for inference\n",
    "class Agent():\n",
    "    def __init__(\n",
    "            self,\n",
    "            config: AgentConfig,\n",
    "            tokenizer: AutoTokenizer,\n",
    "            device: torch.device,\n",
    "        ):\n",
    "\n",
    "        self.model_name = config.model_name\n",
    "\n",
    "        self.inference_engine = LLM(\n",
    "            model=config.model_name,\n",
    "            skip_tokenizer_init=False,\n",
    "            gpu_memory_utilization=0.2,\n",
    "            enable_prefix_caching=True,\n",
    "            swap_space=4,\n",
    "            scheduling_policy=\"fcfs\",\n",
    "            dtype=torch.bfloat16,\n",
    "            max_model_len=config.max_new_tokens * 1.5,\n",
    "            enable_sleep_mode=True,\n",
    "        )\n",
    "                \n",
    "        self.inference_engine.sleep(1)\n",
    "\n",
    "        # hyperparameters for sampling\n",
    "        self.gen_per_sample = config.gen_per_sample\n",
    "        self.temperature = config.temperature\n",
    "        self.max_new_tokens = config.max_new_tokens\n",
    "        self.top_k = config.top_k\n",
    "        self.top_p = config.top_p\n",
    "        self.num_rollouts = config.num_rollouts\n",
    "        self.kl_coefficient = config.kl_coefficient\n",
    "        self.n_rollouts = config.n_rollouts\n",
    "        self.batch_size = config.batch_size\n",
    "        self.learning_rate = config.learning_rate\n",
    "\n",
    "        \n",
    "        deepspeed_config = {\n",
    "            \"bf16\": {\"enabled\": True},\n",
    "            \"zero_optimization\": {\"stage\": 2, \"overlap_comm\": False},\n",
    "            \"train_batch_size\": self.num_rollouts * self.gen_per_sample,\n",
    "            \"train_micro_batch_size_per_gpu\": self.batch_size,\n",
    "            \"gradient_accumulation_steps\": (self.num_rollouts * self.gen_per_sample)// self.batch_size,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \"optimizer\": {\n",
    "                \"type\": \"AdamW\",\n",
    "                \"params\": {\n",
    "                    \"lr\": self.learning_rate,\n",
    "                    \"betas\": (0.9, 0.999),\n",
    "                    \"eps\": 1e-8,\n",
    "                    \"weight_decay\": 0.0,\n",
    "                    \"torch_adam\": True,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        # DeepSpeed config for the reference model\n",
    "        ref_deepspeed_config = {\n",
    "            \"bf16\": {\"enabled\": True},\n",
    "            # Note that we don't train the reference model\n",
    "            # These are just for compatibility with DeepSpeed.\n",
    "            \"train_batch_size\": self.num_rollouts * self.gen_per_sample,\n",
    "\n",
    "            \"train_micro_batch_size_per_gpu\": self.batch_size,\n",
    "            \"gradient_accumulation_steps\": (self.num_rollouts * self.gen_per_sample) // self.batch_size,\n",
    "        }\n",
    "        \n",
    "        self.policy = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            device_map=0,\n",
    "        )\n",
    "        self.reference = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            device_map=0,\n",
    "        )\n",
    "        \n",
    "        self.policy.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "        self.policy, *_ = deepspeed.initialize(\n",
    "            model=self.policy,\n",
    "            config=deepspeed_config,\n",
    "            model_parameters=self.policy.parameters(),\n",
    "        )\n",
    "        self.reference, *_ = deepspeed.initialize(\n",
    "            model=self.reference,\n",
    "            config=ref_deepspeed_config,\n",
    "        )\n",
    "\n",
    "        self.reference.module.cpu()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "        self.pad_token_id = self.eos_token_id\n",
    "        self.ignore_idx = -100\n",
    "\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        wandb.init(\n",
    "            project=\"llm-rl\",\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    def expand_list(self, d_list, rep) -> List[Any]:\n",
    "        return [item for item in d_list for _ in range(rep)]\n",
    "\n",
    "    def rollout(\n",
    "        self,\n",
    "        samples: Dict[str, Any],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "\n",
    "        inputs = [TokensPrompt(prompt_token_ids=ids) for ids in samples[\"prompt_token_ids\"]]\n",
    "\n",
    "        sp = SamplingParams(\n",
    "            n=self.gen_per_sample,\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            top_k=self.top_k,\n",
    "            max_tokens=self.max_new_tokens,\n",
    "            stop_token_ids=[self.eos_token_id],\n",
    "        )\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(1)\n",
    "\n",
    "        self.inference_engine.wake_up()\n",
    "        rollouts = self.inference_engine.generate(inputs, sampling_params=sp)\n",
    "        self.inference_engine.sleep(1)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(1)\n",
    "\n",
    "        gen_token_ids = [out.token_ids for req in rollouts for out in req.outputs]\n",
    "        finish_reasons = [out.finish_reason for req in rollouts for out in req.outputs]\n",
    "\n",
    "        responses = self.tokenizer.batch_decode(gen_token_ids, skip_special_tokens=False)\n",
    "\n",
    "        rollouts = {\n",
    "            \"nums\": self.expand_list(samples[\"nums\"], self.gen_per_sample),\n",
    "            \"target\": self.expand_list(samples[\"target\"], self.gen_per_sample),\n",
    "            \"prompt\": self.expand_list(samples[\"prompt\"], self.gen_per_sample),\n",
    "            \"prompt_token_ids\": self.expand_list(samples[\"prompt_token_ids\"], self.gen_per_sample),\n",
    "            \"gen_token_ids\": gen_token_ids,\n",
    "            \"response\": responses,\n",
    "            \"finish_reason\": finish_reasons,\n",
    "        }\n",
    "\n",
    "        return rollouts\n",
    "\n",
    "    def prep_model_inputs(\n",
    "            self,\n",
    "            rollouts: Dict[str, Any],\n",
    "            pad_id: int = 0,\n",
    "        ) -> Dict[str, Any]:\n",
    "        # --- sanity checks ---\n",
    "        assert len(rollouts[\"prompt_token_ids\"]) == len(rollouts[\"gen_token_ids\"]) \\\n",
    "            == len(rollouts[\"target\"]) == len(rollouts[\"nums\"]) \\\n",
    "            == len(rollouts[\"finish_reason\"])\n",
    "        num_rollouts = len(rollouts[\"prompt_token_ids\"])\n",
    "\n",
    "        # --- sequence lengths ---\n",
    "        max_seq_len = max(\n",
    "            len(p) + len(g)\n",
    "            for p, g in zip(rollouts[\"prompt_token_ids\"], rollouts[\"gen_token_ids\"])\n",
    "        )\n",
    "\n",
    "        # --- rewards ---\n",
    "        rewards_and_metrics = [\n",
    "            compute_reward(resp, {\"nums\": nums, \"target\": tgt})\n",
    "            for resp, nums, tgt in zip(rollouts[\"response\"], rollouts[\"nums\"], rollouts[\"target\"])\n",
    "        ]\n",
    "        rewards, metrics = zip(*rewards_and_metrics)\n",
    "        assert len(rewards) == num_rollouts\n",
    "\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        \n",
    "        # for logging\n",
    "        f_rewards = np.array([m[\"format_reward\"] for m in metrics], dtype=np.float32)\n",
    "        e_rewards = np.array([m[\"equation_reward\"] for m in metrics], dtype=np.float32)\n",
    "        finished_reasons = np.array(\n",
    "            [1 if r == \"stop\" else 0 for r in rollouts[\"finish_reason\"]],\n",
    "            dtype=int\n",
    "        )\n",
    "    \n",
    "        wandb.log({\n",
    "            \"mean_reward\": np.mean(rewards),\n",
    "            \"mean_f_reward\": np.mean(f_rewards),\n",
    "            \"mean_q_reward\": np.mean(e_rewards),\n",
    "            \"frac_finished\": np.sum(finished_reasons) / len(finished_reasons),\n",
    "            \"avg_gen_len\": np.mean([len(g) for g in rollouts[\"gen_token_ids\"]]),\n",
    "        })\n",
    " \n",
    "        # --- group-normalized advantages per rollout ---\n",
    "        adv_chunks = []\n",
    "        for i in range(0, num_rollouts, self.gen_per_sample):\n",
    "            r_group = rewards[i:i + self.gen_per_sample]\n",
    "            group_adv = (r_group - r_group.mean()) / (r_group.std() + 1e-4)\n",
    "            adv_chunks.append(group_adv)\n",
    "        per_rollout_adv = np.concatenate(adv_chunks, axis=0)  # shape: [num_rollouts]\n",
    "        assert per_rollout_adv.shape[0] == num_rollouts\n",
    "\n",
    "        device = self.device\n",
    "        # === create output tensors ===\n",
    "        input_token_ids = torch.full(\n",
    "            (num_rollouts, max_seq_len),\n",
    "            fill_value=pad_id,\n",
    "            dtype=torch.long,\n",
    "            device=device,\n",
    "        )\n",
    "        attn_mask = torch.zeros(\n",
    "            (num_rollouts, max_seq_len),\n",
    "            dtype=torch.bool,\n",
    "            device=device,\n",
    "        )\n",
    "        labels = torch.full(\n",
    "            (num_rollouts, max_seq_len),\n",
    "            fill_value=self.ignore_idx,\n",
    "            dtype=torch.long,\n",
    "            device=device,\n",
    "        )\n",
    "        labels_mask = torch.zeros(\n",
    "            (num_rollouts, max_seq_len),\n",
    "            dtype=torch.bool,\n",
    "            device=device,\n",
    "        )\n",
    "        advantages = torch.zeros(\n",
    "            (num_rollouts, max_seq_len),\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # --- fill rows ---\n",
    "        for idx, (p_ids, g_ids, roll_adv) in enumerate(\n",
    "            zip(rollouts[\"prompt_token_ids\"], rollouts[\"gen_token_ids\"], per_rollout_adv)\n",
    "        ):\n",
    "            g_ids = list(g_ids)\n",
    "            seq = p_ids + g_ids\n",
    "            seq_len = len(seq)\n",
    "            p_len = len(p_ids)\n",
    "            g_len = len(g_ids)\n",
    "\n",
    "            # input ids\n",
    "            input_token_ids[idx, :seq_len] = torch.tensor(seq, dtype=torch.long, device=device)\n",
    "            # attention mask: attend to real tokens only\n",
    "            attn_mask[idx, :seq_len] = True\n",
    "            # labels: only the generated part counts; prompt part is padded/ignored\n",
    "            labels[idx, p_len:p_len+g_len] = torch.tensor(g_ids, dtype=torch.long, device=device)\n",
    "            labels_mask[idx, p_len:p_len+g_len] = True\n",
    "            # per-token advantages on generated region (same scalar across the gen span)\n",
    "            if g_len > 0:\n",
    "                advantages[idx, p_len:p_len+g_len] = float(roll_adv)\n",
    "\n",
    "        return {\n",
    "            \"input_token_ids\": input_token_ids,\n",
    "            \"attn_mask\": attn_mask,\n",
    "            \"advantages\": advantages,\n",
    "            \"labels\": labels,\n",
    "            \"labels_mask\": labels_mask,\n",
    "            \"finish_reason\": rollouts[\"finish_reason\"]\n",
    "        }\n",
    "    \n",
    "    def log_probs(self, model, model_inputs, temperature) -> torch.Tensor:\n",
    "        outputs = model(\n",
    "            input_ids=model_inputs[\"input_token_ids\"],\n",
    "            attention_mask=model_inputs[\"attn_mask\"],\n",
    "            return_dict=True,\n",
    "            use_cache=False,\n",
    "        )\n",
    "        # temperature scaling (make sure temperature > 0)\n",
    "        logits = outputs.logits / temperature\n",
    "    \n",
    "        # shift for next-token prediction\n",
    "        shift_logits = logits[..., :-1, :]\n",
    "        shift_labels = model_inputs[\"labels\"][..., 1:]                # [B, T-1]\n",
    "        shift_labels_mask = model_inputs[\"labels_mask\"][..., 1:]      # [B, T-1]\n",
    "    \n",
    "        # log softmax over vocab\n",
    "        vocab_log_probs = shift_logits.log_softmax(dim=-1)            # [B, T-1, V]\n",
    "    \n",
    "        # SAFELY gather: replace ignored labels with 0 (or any valid token id)\n",
    "        safe_labels = torch.where(\n",
    "            shift_labels_mask.bool(),\n",
    "            shift_labels,\n",
    "            torch.zeros_like(shift_labels)\n",
    "        )\n",
    "        token_logps = torch.gather(vocab_log_probs, dim=-1,\n",
    "                                   index=safe_labels.unsqueeze(-1)).squeeze(-1)  # [B, T-1]\n",
    "    \n",
    "        # mask out ignored positions\n",
    "        token_logps = token_logps * shift_labels_mask\n",
    "        return token_logps  # [B, T-1] masked\n",
    "    \n",
    "    def compute_loss(self, batch: Dict[str, torch.Tensor], total_response_len: int):\n",
    "        input_token_ids = batch[\"input_token_ids\"]\n",
    "        attention_mask  = batch[\"attn_mask\"]\n",
    "        labels          = batch[\"labels\"]\n",
    "        labels_mask     = batch[\"labels_mask\"]\n",
    "        advantages      = batch[\"advantages\"]\n",
    "    \n",
    "        assert torch.all(labels_mask == (labels != self.ignore_idx))\n",
    "    \n",
    "        model_inputs = {\n",
    "            \"input_token_ids\": input_token_ids,\n",
    "            \"attn_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"labels_mask\": labels_mask,\n",
    "        }\n",
    "    \n",
    "        self.reference.to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            ref_logps = self.log_probs(self.reference, model_inputs, self.temperature)  # [B, T-1]\n",
    "    \n",
    "        logps = self.log_probs(self.policy, model_inputs, self.temperature)             # [B, T-1]\n",
    "    \n",
    "        # shift masks/advantages once to match [B, T-1]\n",
    "        m = labels_mask[..., 1:]             # [B, T-1]\n",
    "        adv = advantages[..., 1:]            # [B, T-1]\n",
    "    \n",
    "        # KL term (token-wise) using log-probs\n",
    "        # d = ref_logps - logps;  exp(d) - d - 1 is a symmetric KL-like penalty\n",
    "        d = ref_logps - logps\n",
    "        kl_penalty = (torch.exp(d) - d - 1) * m\n",
    "    \n",
    "        # Entropy: negative mean log-prob on valid positions (optional: compute from full dist)\n",
    "        entropy = -(logps.sum() / (m.sum().clamp_min(1)))\n",
    "    \n",
    "        # Policy loss (masked)\n",
    "        policy_loss = -(logps * adv) * m\n",
    "    \n",
    "        loss = (policy_loss.sum() + self.kl_coefficient * kl_penalty.sum()) / max(total_response_len, 1)\n",
    "    \n",
    "        metrics = {\n",
    "            \"policy_loss\": (policy_loss.sum().item() / max(total_response_len, 1)),\n",
    "            \"kl_penalty\": (kl_penalty.sum().item() / max(total_response_len, 1)),\n",
    "            \"entropy\": float(entropy.item()),\n",
    "        }\n",
    "        return loss, metrics\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        train_ds,\n",
    "        test_ds=None,\n",
    "        num_epochs: int = 1,\n",
    "        eval_every: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        For each epoch:\n",
    "          - Sample self.n_rollouts prompts\n",
    "          - Generate self.gen_per_sample responses each (vLLM)\n",
    "          - Build tensors\n",
    "          - Accumulate grads over ALL mini-batches from this rollout\n",
    "          - Do ONE optimizer step (and weight clip)\n",
    "          - Refresh vLLM weights\n",
    "        \"\"\"\n",
    "    \n",
    "        history = {\"loss\": [], \"policy_loss\": [], \"kl_penalty\": [], \"entropy\": []}\n",
    "    \n",
    "        def _sample_batch(ds, n_items: int):\n",
    "            import random\n",
    "            if hasattr(ds, \"select\"):\n",
    "                idxs = random.sample(range(len(ds)), n_items)\n",
    "                return ds.select(idxs)\n",
    "            else:\n",
    "                idxs = random.sample(range(len(ds[\"prompt_token_ids\"])), n_items)\n",
    "                return {k: [ds[k][i] for i in idxs] for k in ds.keys()}\n",
    "    \n",
    "        global_step = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            # 1) Sample prompts for this rollout batch\n",
    "            batch_prompts = _sample_batch(train_ds, self.n_rollouts)\n",
    "    \n",
    "            # 2) Generate rollouts (expands rows by gen_per_sample)\n",
    "            rollouts = self.rollout(batch_prompts)\n",
    "    \n",
    "            # 3) Pack tensors\n",
    "            data = self.prep_model_inputs(rollouts, pad_id=self.pad_token_id)\n",
    "            training_size = len(rollouts[\"prompt_token_ids\"])  # = n_rollouts * gen_per_sample\n",
    "            assert training_size == self.n_rollouts * self.gen_per_sample\n",
    "\n",
    "            self.policy.to(self.device)\n",
    "            self.policy.train()\n",
    "            self.reference.to(self.device)\n",
    "            self.reference.eval()\n",
    "            \n",
    "            # 4) Accumulate gradients over ALL mini-batches, then do ONE step\n",
    "    \n",
    "            accum_loss_value = 0.0\n",
    "            accum_metrics = {\"policy_loss\": 0.0, \"kl_penalty\": 0.0, \"entropy\": 0.0}\n",
    "    \n",
    "            for b_start in tqdm(range(0, training_size, self.batch_size)):\n",
    "                b_end = min(b_start + self.batch_size, training_size)\n",
    "    \n",
    "                batch = {\n",
    "                    \"input_token_ids\": data[\"input_token_ids\"][b_start:b_end],\n",
    "                    \"attn_mask\":       data[\"attn_mask\"][b_start:b_end],\n",
    "                    \"labels\":          data[\"labels\"][b_start:b_end],\n",
    "                    \"labels_mask\":     data[\"labels_mask\"][b_start:b_end],\n",
    "                    \"advantages\":      data[\"advantages\"][b_start:b_end],\n",
    "                }\n",
    "    \n",
    "                # match shift in log_probs: count valid positions post-shift\n",
    "                total_response_len = int(batch[\"labels_mask\"][..., 1:].sum().item())\n",
    "    \n",
    "                loss, metrics = self.compute_loss(batch, total_response_len)\n",
    "\n",
    "                # deepspeed backward\n",
    "                self.policy.backward(loss, scale_wrt_gas=False)\n",
    "                \n",
    "                wandb.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"policy_loss\": metrics[\"policy_loss\"],\n",
    "                    \"kl_penalty\": metrics[\"kl_penalty\"],\n",
    "                    \"entropy\": metrics[\"entropy\"],\n",
    "                })\n",
    "                \n",
    "\n",
    "                accum_loss_value += float(loss.item())\n",
    "                for k in accum_metrics:\n",
    "                    accum_metrics[k] += metrics[k]\n",
    "\n",
    "                del loss, metrics\n",
    "               \n",
    "                global_step += 1\n",
    "\n",
    "                if self.policy.is_gradient_accumulation_boundary():\n",
    "                    self.reference.module.cpu()\n",
    "\n",
    "                self.policy.step()\n",
    "                \n",
    "                if eval_every and (global_step % eval_every == 0) and (test_ds is not None):\n",
    "                    self.policy.eval()\n",
    "                    # TODO: add quick eval\n",
    "                    self.policy.train()\n",
    "    \n",
    "    \n",
    "            # Log epoch-averaged metrics\n",
    "            history[\"loss\"].append(accum_loss_value)\n",
    "            history[\"policy_loss\"].append(accum_metrics[\"policy_loss\"])\n",
    "            history[\"kl_penalty\"].append(accum_metrics[\"kl_penalty\"])\n",
    "            history[\"entropy\"].append(accum_metrics[\"entropy\"])\n",
    "\n",
    "            print(f\"[{epoch}/{num_epochs}]  loss: {accum_loss_value}, policy_loss: {accum_metrics['policy_loss']}, kl-penalty: {accum_metrics['kl_penalty']}, entropy:{accum_metrics['entropy']}\")\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            time.sleep(1)\n",
    "            # 5) Push updated weights into vLLM\n",
    "            self.update_inference_engine()\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            time.sleep(1)\n",
    "    \n",
    "        return history\n",
    "\n",
    "    def update_inference_engine(self) -> None:\n",
    "        \"\"\"\n",
    "        Update the running vLLM inference engine with the latest weights\n",
    "        from the training policy model.\n",
    "        Assumes inference engine sleeps when called\n",
    "        After call inference engine is ready to be used (i.e. awake)\n",
    "    \n",
    "        This function:\n",
    "          1. Wake up vllm inference engine\n",
    "          2. Copies the current HF model weights from self.policy.\n",
    "          3. Pushes them into the vLLM engine without restarting.\n",
    "    \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- wake the engine and push weights into the internal model ---\n",
    "        self.inference_engine.wake_up()\n",
    "        \n",
    "        device = \"cuda:0\"\n",
    "        \n",
    "        # find the V0 path and call load_weights\n",
    "        model_obj = getattr(\n",
    "            getattr(\n",
    "                getattr(self.inference_engine.llm_engine, \"model_executor\").driver_worker,\n",
    "                \"model_runner\"\n",
    "            ),\n",
    "            \"model\"\n",
    "        )\n",
    "        state_dict = self.policy.module.state_dict() if isinstance(self.policy, DeepSpeedEngine) else model.state_dict()\n",
    "\n",
    "        model_obj.load_weights(weights=state_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:07:17 [config.py:717] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 09-17 23:07:17 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-3B', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-3B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 09-17 23:07:18 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 09-17 23:07:19 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 09-17 23:07:19 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-3B...\n",
      "INFO 09-17 23:07:19 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54db19e392c94b1890c10edd283024b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:07:20 [loader.py:458] Loading weights took 1.23 seconds\n",
      "INFO 09-17 23:07:21 [model_runner.py:1140] Model loading took 5.7916 GiB and 1.569475 seconds\n",
      "INFO 09-17 23:07:22 [worker.py:287] Memory profiling takes 0.70 seconds\n",
      "INFO 09-17 23:07:22 [worker.py:287] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.20) = 15.85GiB\n",
      "INFO 09-17 23:07:22 [worker.py:287] model weights take 5.79GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.57GiB.\n",
      "INFO 09-17 23:07:22 [executor_base.py:112] # cuda blocks: 15608, # CPU blocks: 7281\n",
      "INFO 09-17 23:07:22 [executor_base.py:117] Maximum concurrency for 1536 tokens per request: 162.58x\n",
      "INFO 09-17 23:07:28 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa587aab8b24e22865cf113d2c153d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:07:55 [model_runner.py:1592] Graph capturing finished in 26 secs, took 0.23 GiB\n",
      "INFO 09-17 23:07:55 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 34.03 seconds\n",
      "INFO 09-17 23:07:55 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:07:55 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:07:59 [worker.py:145] Sleep mode freed 14.44 GiB memory, 0.78 GiB memory is still in use.\n",
      "INFO 09-17 23:07:59 [executor_base.py:210] It took 3.405036 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b131e6fc15f44dd99828f2f7a51042a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa64d57be507456da279f4f00a7c17b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-17 23:08:02,797] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-09-17 23:08:02,802] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-09-17 23:08:02,803] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n",
      "[2025-09-17 23:08:03,049] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-09-17 23:08:03,052] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-09-17 23:08:03,053] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-09-17 23:08:03,077] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-09-17 23:08:03,078] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2025-09-17 23:08:03,079] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2025-09-17 23:08:03,080] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000\n",
      "[2025-09-17 23:08:03,080] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000\n",
      "[2025-09-17 23:08:03,080] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2025-09-17 23:08:03,081] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2025-09-17 23:08:46,899] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-09-17 23:08:46,901] [INFO] [utils.py:782:see_memory_usage] MA 43.14 GB         Max_MA 48.89 GB         CA 61.54 GB         Max_CA 62 GB \n",
      "[2025-09-17 23:08:46,903] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 78.23 GB, percent = 8.3%\n",
      "[2025-09-17 23:08:47,294] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-09-17 23:08:47,296] [INFO] [utils.py:782:see_memory_usage] MA 43.14 GB         Max_MA 54.64 GB         CA 73.04 GB         Max_CA 73 GB \n",
      "[2025-09-17 23:08:47,297] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 78.23 GB, percent = 8.3%\n",
      "[2025-09-17 23:08:47,298] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized\n",
      "[2025-09-17 23:08:47,685] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-09-17 23:08:47,687] [INFO] [utils.py:782:see_memory_usage] MA 43.14 GB         Max_MA 43.14 GB         CA 73.04 GB         Max_CA 73 GB \n",
      "[2025-09-17 23:08:47,688] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 78.24 GB, percent = 8.3%\n",
      "[2025-09-17 23:08:47,696] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-09-17 23:08:47,696] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-09-17 23:08:47,697] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-09-17 23:08:47,698] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-06], mom=[(0.9, 0.999)]\n",
      "[2025-09-17 23:08:47,699] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:\n",
      "[2025-09-17 23:08:47,700] [INFO] [config.py:1005:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-09-17 23:08:47,701] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-09-17 23:08:47,702] [INFO] [config.py:1005:print]   amp_enabled .................. False\n",
      "[2025-09-17 23:08:47,702] [INFO] [config.py:1005:print]   amp_params ................... False\n",
      "[2025-09-17 23:08:47,703] [INFO] [config.py:1005:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-09-17 23:08:47,704] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True\n",
      "[2025-09-17 23:08:47,704] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-09-17 23:08:47,705] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-09-17 23:08:47,705] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-09-17 23:08:47,706] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-09-17 23:08:47,706] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5d5c1c6f90>\n",
      "[2025-09-17 23:08:47,707] [INFO] [config.py:1005:print]   communication_data_type ...... None\n",
      "[2025-09-17 23:08:47,707] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-09-17 23:08:47,709] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False\n",
      "[2025-09-17 23:08:47,709] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False\n",
      "[2025-09-17 23:08:47,710] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-09-17 23:08:47,710] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False\n",
      "[2025-09-17 23:08:47,711] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False\n",
      "[2025-09-17 23:08:47,711] [INFO] [config.py:1005:print]   disable_allgather ............ False\n",
      "[2025-09-17 23:08:47,711] [INFO] [config.py:1005:print]   dump_state ................... False\n",
      "[2025-09-17 23:08:47,712] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-09-17 23:08:47,712] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False\n",
      "[2025-09-17 23:08:47,712] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-09-17 23:08:47,713] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-09-17 23:08:47,713] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-09-17 23:08:47,714] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-09-17 23:08:47,714] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-09-17 23:08:47,714] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-09-17 23:08:47,715] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False\n",
      "[2025-09-17 23:08:47,715] [INFO] [config.py:1005:print]   elasticity_enabled ........... False\n",
      "[2025-09-17 23:08:47,716] [INFO] [config.py:1005:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-09-17 23:08:47,716] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None\n",
      "[2025-09-17 23:08:47,716] [INFO] [config.py:1005:print]   fp16_enabled ................. False\n",
      "[2025-09-17 23:08:47,717] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-09-17 23:08:47,717] [INFO] [config.py:1005:print]   global_rank .................. 0\n",
      "[2025-09-17 23:08:47,718] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None\n",
      "[2025-09-17 23:08:47,718] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 16\n",
      "[2025-09-17 23:08:47,718] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0\n",
      "[2025-09-17 23:08:47,720] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-09-17 23:08:47,721] [INFO] [config.py:1005:print]   graph_harvesting ............. False\n",
      "[2025-09-17 23:08:47,721] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-09-17 23:08:47,722] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1\n",
      "[2025-09-17 23:08:47,722] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False\n",
      "[2025-09-17 23:08:47,723] [INFO] [config.py:1005:print]   loss_scale ................... 1.0\n",
      "[2025-09-17 23:08:47,723] [INFO] [config.py:1005:print]   memory_breakdown ............. False\n",
      "[2025-09-17 23:08:47,723] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False\n",
      "[2025-09-17 23:08:47,724] [INFO] [config.py:1005:print]   mics_shard_size .............. -1\n",
      "[2025-09-17 23:08:47,724] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-09-17 23:08:47,725] [INFO] [config.py:1005:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-09-17 23:08:47,725] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-09-17 23:08:47,725] [INFO] [config.py:1005:print]   optimizer_name ............... adamw\n",
      "[2025-09-17 23:08:47,726] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 1e-06, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0}\n",
      "[2025-09-17 23:08:47,726] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-09-17 23:08:47,727] [INFO] [config.py:1005:print]   pld_enabled .................. False\n",
      "[2025-09-17 23:08:47,727] [INFO] [config.py:1005:print]   pld_params ................... False\n",
      "[2025-09-17 23:08:47,727] [INFO] [config.py:1005:print]   prescale_gradients ........... False\n",
      "[2025-09-17 23:08:47,728] [INFO] [config.py:1005:print]   scheduler_name ............... None\n",
      "[2025-09-17 23:08:47,728] [INFO] [config.py:1005:print]   scheduler_params ............. None\n",
      "[2025-09-17 23:08:47,729] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-09-17 23:08:47,729] [INFO] [config.py:1005:print]   sparse_attention ............. None\n",
      "[2025-09-17 23:08:47,729] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False\n",
      "[2025-09-17 23:08:47,730] [INFO] [config.py:1005:print]   steps_per_print .............. None\n",
      "[2025-09-17 23:08:47,730] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-09-17 23:08:47,730] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-09-17 23:08:47,731] [INFO] [config.py:1005:print]   train_batch_size ............. 64\n",
      "[2025-09-17 23:08:47,731] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4\n",
      "[2025-09-17 23:08:47,732] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False\n",
      "[2025-09-17 23:08:47,732] [INFO] [config.py:1005:print]   use_node_local_storage ....... False\n",
      "[2025-09-17 23:08:47,732] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False\n",
      "[2025-09-17 23:08:47,733] [INFO] [config.py:1005:print]   weight_quantization_config ... None\n",
      "[2025-09-17 23:08:47,733] [INFO] [config.py:1005:print]   world_size ................... 1\n",
      "[2025-09-17 23:08:47,733] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False\n",
      "[2025-09-17 23:08:47,734] [INFO] [config.py:1005:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-09-17 23:08:47,734] [INFO] [config.py:1005:print]   zero_enabled ................. True\n",
      "[2025-09-17 23:08:47,735] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-09-17 23:08:47,735] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 2\n",
      "[2025-09-17 23:08:47,736] [INFO] [config.py:991:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"overlap_comm\": false\n",
      "    }, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 16, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 1e-06, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.0\n",
      "        }\n",
      "    }\n",
      "}\n",
      "[2025-09-17 23:08:47,736] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-09-17 23:08:47,737] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n",
      "[2025-09-17 23:08:47,781] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-09-17 23:08:47,783] [INFO] [logging.py:128:log_dist] [Rank 0] Creating BF16 optimizer\n",
      "[2025-09-17 23:08:48,188] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer\n",
      "[2025-09-17 23:08:48,191] [INFO] [utils.py:782:see_memory_usage] MA 37.38 GB         Max_MA 43.72 GB         CA 73.11 GB         Max_CA 73 GB \n",
      "[2025-09-17 23:08:48,192] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 78.24 GB, percent = 8.3%\n",
      "[2025-09-17 23:08:48,579] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer\n",
      "[2025-09-17 23:08:48,581] [INFO] [utils.py:782:see_memory_usage] MA 37.38 GB         Max_MA 37.38 GB         CA 73.11 GB         Max_CA 73 GB \n",
      "[2025-09-17 23:08:48,583] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 78.24 GB, percent = 8.3%\n",
      "[2025-09-17 23:08:48,584] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:\n",
      "[2025-09-17 23:08:48,586] [INFO] [config.py:1005:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-09-17 23:08:48,586] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-09-17 23:08:48,587] [INFO] [config.py:1005:print]   amp_enabled .................. False\n",
      "[2025-09-17 23:08:48,587] [INFO] [config.py:1005:print]   amp_params ................... False\n",
      "[2025-09-17 23:08:48,588] [INFO] [config.py:1005:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-09-17 23:08:48,589] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True\n",
      "[2025-09-17 23:08:48,589] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-09-17 23:08:48,589] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-09-17 23:08:48,590] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-09-17 23:08:48,590] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-09-17 23:08:48,591] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5ca42ae910>\n",
      "[2025-09-17 23:08:48,591] [INFO] [config.py:1005:print]   communication_data_type ...... None\n",
      "[2025-09-17 23:08:48,591] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-09-17 23:08:48,592] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False\n",
      "[2025-09-17 23:08:48,592] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False\n",
      "[2025-09-17 23:08:48,593] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-09-17 23:08:48,593] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False\n",
      "[2025-09-17 23:08:48,594] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False\n",
      "[2025-09-17 23:08:48,594] [INFO] [config.py:1005:print]   disable_allgather ............ False\n",
      "[2025-09-17 23:08:48,595] [INFO] [config.py:1005:print]   dump_state ................... False\n",
      "[2025-09-17 23:08:48,595] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-09-17 23:08:48,596] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False\n",
      "[2025-09-17 23:08:48,596] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-09-17 23:08:48,597] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-09-17 23:08:48,597] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-09-17 23:08:48,598] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-09-17 23:08:48,598] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-09-17 23:08:48,599] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-09-17 23:08:48,599] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False\n",
      "[2025-09-17 23:08:48,600] [INFO] [config.py:1005:print]   elasticity_enabled ........... False\n",
      "[2025-09-17 23:08:48,600] [INFO] [config.py:1005:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-09-17 23:08:48,601] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None\n",
      "[2025-09-17 23:08:48,601] [INFO] [config.py:1005:print]   fp16_enabled ................. False\n",
      "[2025-09-17 23:08:48,602] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-09-17 23:08:48,602] [INFO] [config.py:1005:print]   global_rank .................. 0\n",
      "[2025-09-17 23:08:48,603] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None\n",
      "[2025-09-17 23:08:48,603] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 16\n",
      "[2025-09-17 23:08:48,603] [INFO] [config.py:1005:print]   gradient_clipping ............ 0.0\n",
      "[2025-09-17 23:08:48,604] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-09-17 23:08:48,604] [INFO] [config.py:1005:print]   graph_harvesting ............. False\n",
      "[2025-09-17 23:08:48,604] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-09-17 23:08:48,605] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1\n",
      "[2025-09-17 23:08:48,605] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False\n",
      "[2025-09-17 23:08:48,606] [INFO] [config.py:1005:print]   loss_scale ................... 1.0\n",
      "[2025-09-17 23:08:48,606] [INFO] [config.py:1005:print]   memory_breakdown ............. False\n",
      "[2025-09-17 23:08:48,606] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False\n",
      "[2025-09-17 23:08:48,607] [INFO] [config.py:1005:print]   mics_shard_size .............. -1\n",
      "[2025-09-17 23:08:48,607] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-09-17 23:08:48,608] [INFO] [config.py:1005:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-09-17 23:08:48,608] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-09-17 23:08:48,608] [INFO] [config.py:1005:print]   optimizer_name ............... None\n",
      "[2025-09-17 23:08:48,612] [INFO] [config.py:1005:print]   optimizer_params ............. None\n",
      "[2025-09-17 23:08:48,612] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-09-17 23:08:48,613] [INFO] [config.py:1005:print]   pld_enabled .................. False\n",
      "[2025-09-17 23:08:48,613] [INFO] [config.py:1005:print]   pld_params ................... False\n",
      "[2025-09-17 23:08:48,613] [INFO] [config.py:1005:print]   prescale_gradients ........... False\n",
      "[2025-09-17 23:08:48,614] [INFO] [config.py:1005:print]   scheduler_name ............... None\n",
      "[2025-09-17 23:08:48,614] [INFO] [config.py:1005:print]   scheduler_params ............. None\n",
      "[2025-09-17 23:08:48,615] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-09-17 23:08:48,615] [INFO] [config.py:1005:print]   sparse_attention ............. None\n",
      "[2025-09-17 23:08:48,615] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False\n",
      "[2025-09-17 23:08:48,616] [INFO] [config.py:1005:print]   steps_per_print .............. None\n",
      "[2025-09-17 23:08:48,616] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-09-17 23:08:48,617] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-09-17 23:08:48,617] [INFO] [config.py:1005:print]   train_batch_size ............. 64\n",
      "[2025-09-17 23:08:48,617] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4\n",
      "[2025-09-17 23:08:48,618] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False\n",
      "[2025-09-17 23:08:48,618] [INFO] [config.py:1005:print]   use_node_local_storage ....... False\n",
      "[2025-09-17 23:08:48,618] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False\n",
      "[2025-09-17 23:08:48,619] [INFO] [config.py:1005:print]   weight_quantization_config ... None\n",
      "[2025-09-17 23:08:48,619] [INFO] [config.py:1005:print]   world_size ................... 1\n",
      "[2025-09-17 23:08:48,619] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False\n",
      "[2025-09-17 23:08:48,620] [INFO] [config.py:1005:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-09-17 23:08:48,620] [INFO] [config.py:1005:print]   zero_enabled ................. False\n",
      "[2025-09-17 23:08:48,621] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-09-17 23:08:48,621] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 0\n",
      "[2025-09-17 23:08:48,621] [INFO] [config.py:991:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 16\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaulkroe\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250917_230852-8w7yh3ed</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/paulkroe/llm-rl/runs/8w7yh3ed' target=\"_blank\">balmy-terrain-25</a></strong> to <a href='https://wandb.ai/paulkroe/llm-rl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/paulkroe/llm-rl' target=\"_blank\">https://wandb.ai/paulkroe/llm-rl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/paulkroe/llm-rl/runs/8w7yh3ed' target=\"_blank\">https://wandb.ai/paulkroe/llm-rl/runs/8w7yh3ed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:08:53 [executor_base.py:226] It took 0.272059 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:08:54 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af54d388421045c3aa934febe94fe6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:09:13 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:09:13 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:09:14 [worker.py:145] Sleep mode freed 15.44 GiB memory, 18.14 GiB memory is still in use.\n",
      "INFO 09-17 23:09:14 [executor_base.py:210] It took 0.988176 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70741eaf154646b8817b5b31723e7c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1000]  loss: -6.901474309717642, policy_loss: -6.901489783205986, kl-penalty: 0.015510419831600314, entropy:87.1328125\n",
      "INFO 09-17 23:10:41 [executor_base.py:226] It took 0.278572 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:10:44 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a5786aec954e80b5654fdb9d1cbff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:11:02 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:11:02 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:11:03 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:11:03 [executor_base.py:210] It took 0.852668 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436f9d58f89e40bc9f5ea54e676d1aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000]  loss: -3.8172612809884043, policy_loss: -3.817294981317409, kl-penalty: 0.03356890001078369, entropy:85.95703125\n",
      "INFO 09-17 23:12:29 [executor_base.py:226] It took 0.310887 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:12:32 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99f64fff441435d8d4f2725cdccac7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:12:50 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:12:50 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:12:51 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:12:51 [executor_base.py:210] It took 0.881866 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636f0c2850924976991c88528ffec198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/1000]  loss: -8.159089388722009, policy_loss: -8.159182479989576, kl-penalty: 0.09318466301826915, entropy:81.6796875\n",
      "INFO 09-17 23:14:17 [executor_base.py:226] It took 0.321871 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:14:20 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ba6d82baed4440952b3322d683da93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:14:36 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:14:36 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:14:37 [worker.py:145] Sleep mode freed 15.32 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:14:37 [executor_base.py:210] It took 0.864546 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726c3a3d9fb442d68be365323f7200d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/1000]  loss: -15.509342431334971, policy_loss: -15.509630410007647, kl-penalty: 0.28802424130780213, entropy:74.673828125\n",
      "INFO 09-17 23:16:02 [executor_base.py:226] It took 0.347944 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:16:05 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2955f9d0384142dd8bb3992b669ccc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:16:20 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:16:20 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:16:21 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:16:21 [executor_base.py:210] It took 0.830059 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a097e585d04237a6d669d83d469073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/1000]  loss: -11.462035398492617, policy_loss: -11.46272781468646, kl-penalty: 0.6920554447018766, entropy:72.2734375\n",
      "INFO 09-17 23:17:48 [executor_base.py:226] It took 0.339311 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:17:51 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34991732c6fe49feaaa82174b026436f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:18:06 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:18:06 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:18:07 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:18:07 [executor_base.py:210] It took 0.860614 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7827f335ddf8465d89b7a96dac191e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/1000]  loss: -13.111384135504522, policy_loss: -13.112450083233046, kl-penalty: 1.0654617137025715, entropy:65.37109375\n",
      "INFO 09-17 23:19:32 [executor_base.py:226] It took 0.335392 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:19:35 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c938349867bd4e98ba13a4392589f40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:19:48 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:19:48 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:19:49 [worker.py:145] Sleep mode freed 15.32 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:19:49 [executor_base.py:210] It took 0.831330 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b56a1e399b645919569be034e5bf254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/1000]  loss: 0.46053531151665084, policy_loss: 0.4588394320467646, kl-penalty: 1.6965370851896902, entropy:66.2265625\n",
      "INFO 09-17 23:21:10 [executor_base.py:226] It took 0.344593 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:21:13 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eabce577c04491aa9879274a8034b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:21:28 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:21:28 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:21:28 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:21:28 [executor_base.py:210] It took 0.849362 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fdfd66b0004b97ac9b76add620fcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/1000]  loss: -10.514788010133998, policy_loss: -10.516552544208007, kl-penalty: 1.764365790883094, entropy:63.55859375\n",
      "INFO 09-17 23:22:53 [executor_base.py:226] It took 0.280019 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:22:56 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2163e7c66149f981dbea6c9746c03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:23:10 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:23:10 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:23:11 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:23:11 [executor_base.py:210] It took 0.826240 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e24b7ed0f4e4318929133ed0e6dd289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/1000]  loss: -2.0534693083664024, policy_loss: -2.055271799583789, kl-penalty: 1.8023077159455292, entropy:64.296875\n",
      "INFO 09-17 23:24:37 [executor_base.py:226] It took 0.346296 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:24:39 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ac9c4f04f64e0faaa16400d2be437b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:24:53 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:24:53 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:24:53 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:24:53 [executor_base.py:210] It took 0.834028 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d028ce9facb4e7b9463aa1fafe7e6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/1000]  loss: 1.855340189970775, policy_loss: 1.8522616946956463, kl-penalty: 3.077616680272718, entropy:64.35546875\n",
      "INFO 09-17 23:26:18 [executor_base.py:226] It took 0.290756 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:26:21 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9fb7eb9a1e432fbd2ebcdc40097bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:26:33 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:26:33 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:26:34 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:26:34 [executor_base.py:210] It took 0.820398 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b3f3e39a1045bb92fc01cab235b892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/1000]  loss: -6.009392825353643, policy_loss: -6.011575228554113, kl-penalty: 2.182040424594987, entropy:64.16796875\n",
      "INFO 09-17 23:27:54 [executor_base.py:226] It took 0.322669 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:27:57 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33684d6753dc4e508b238e7f767a45ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:28:10 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:28:10 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:28:10 [worker.py:145] Sleep mode freed 15.32 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:28:10 [executor_base.py:210] It took 0.850829 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027f6d19caeb42b0ac4195f00fd719ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/1000]  loss: 0.8666265550527896, policy_loss: 0.8612157988256306, kl-penalty: 5.417650995440461, entropy:61.05859375\n",
      "INFO 09-17 23:29:37 [executor_base.py:226] It took 0.326460 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:29:40 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ca7781e8d84c689bc9938a07017a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:29:53 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:29:53 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:29:54 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:29:54 [executor_base.py:210] It took 0.905198 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de36b4d4a2742cb9ff9d13b9bcb3657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/1000]  loss: -2.522842923319331, policy_loss: -2.525269483339642, kl-penalty: 2.427860949770549, entropy:64.49609375\n",
      "INFO 09-17 23:31:20 [executor_base.py:226] It took 0.318838 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:31:23 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606b029f2b0140ce9369d61d05b482bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:31:36 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:31:36 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:31:37 [worker.py:145] Sleep mode freed 15.32 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:31:37 [executor_base.py:210] It took 0.907340 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a137ed755b44edb87c8b293e3eb5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/1000]  loss: -0.8463586469260918, policy_loss: -0.8494863486322812, kl-penalty: 3.127878081676958, entropy:64.515625\n",
      "INFO 09-17 23:33:03 [executor_base.py:226] It took 0.309090 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:33:06 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bcfb146de64584970bbd5c9b79e12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:33:18 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:33:18 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:33:19 [worker.py:145] Sleep mode freed 15.32 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:33:19 [executor_base.py:210] It took 0.798139 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c47616b8c5643e6b49014d810ae426d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/1000]  loss: -2.8848379127375665, policy_loss: -2.8876982476234883, kl-penalty: 2.860396831576261, entropy:64.1640625\n",
      "INFO 09-17 23:34:36 [executor_base.py:226] It took 0.318162 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:34:38 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44366d52c4464a70a92c2f638aca455b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:34:49 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:34:49 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:34:50 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:34:50 [executor_base.py:210] It took 0.865158 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b391e2bc94274b9bb8e9e01efba40b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/1000]  loss: -2.739389882497562, policy_loss: -2.742004068826343, kl-penalty: 2.6146743849106078, entropy:63.70703125\n",
      "INFO 09-17 23:35:56 [executor_base.py:226] It took 0.326465 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:35:59 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fad01e942442a09a9d53dd2a5cabde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:36:11 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:36:12 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:36:12 [worker.py:145] Sleep mode freed 15.32 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:36:12 [executor_base.py:210] It took 0.805993 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9130e606aad40678d3fe9f3384de15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/1000]  loss: -0.18016115964019264, policy_loss: -0.1833034042282642, kl-penalty: 3.142545331482678, entropy:62.6171875\n",
      "INFO 09-17 23:37:39 [executor_base.py:226] It took 0.330991 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:37:42 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53c0637a4d5425f937307448c4ef911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:37:54 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:37:54 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:37:55 [worker.py:145] Sleep mode freed 15.32 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:37:55 [executor_base.py:210] It took 0.846729 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c9ef7c9db54652839d82a53c8e5264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/1000]  loss: -2.7813392970165296, policy_loss: -2.784680934886334, kl-penalty: 3.340900348718832, entropy:57.2578125\n",
      "INFO 09-17 23:39:21 [executor_base.py:226] It took 0.322171 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:39:24 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57ac270b42848dca0804e7ac000fdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:39:35 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:39:35 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:39:35 [worker.py:145] Sleep mode freed 15.31 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:39:35 [executor_base.py:210] It took 0.837904 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a090dbfe824d02a695c2dcb689adea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/1000]  loss: -1.8712133781591547, policy_loss: -1.874757699728032, kl-penalty: 3.545118241131791, entropy:56.04296875\n",
      "INFO 09-17 23:40:48 [executor_base.py:226] It took 0.343993 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:40:51 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500b9a4683514038ad77d7163f2c8d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:41:01 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:41:01 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:41:02 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:41:02 [executor_base.py:210] It took 0.851866 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc69d5063d74e25af6964043984f11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/1000]  loss: -1.4259489094583842, policy_loss: -1.4308803690826049, kl-penalty: 4.931498965690142, entropy:56.46484375\n",
      "INFO 09-17 23:42:12 [executor_base.py:226] It took 0.349304 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:42:15 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cea18ce9a284c3982551c95c1db9276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:42:24 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:42:24 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:42:25 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:42:25 [executor_base.py:210] It took 0.886277 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2178d87170154d3ea1ef838a834f1073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/1000]  loss: -1.8872729907161556, policy_loss: -1.8929966998289047, kl-penalty: 5.725016263444757, entropy:49.88671875\n",
      "INFO 09-17 23:43:33 [executor_base.py:226] It took 0.290393 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:43:36 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ad57cd860c4a4995be8bf2bae1bf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:43:49 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:43:49 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:43:50 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:43:50 [executor_base.py:210] It took 0.816459 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e7a595119143dd90a3cb01258f0313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/1000]  loss: -5.012115724024625, policy_loss: -5.018439542580413, kl-penalty: 6.3249241046758575, entropy:51.400390625\n",
      "INFO 09-17 23:45:16 [executor_base.py:226] It took 0.288541 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:45:19 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f55c636655455eb36098cca8bbc4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:45:31 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:45:31 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:45:32 [worker.py:145] Sleep mode freed 15.31 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:45:32 [executor_base.py:210] It took 0.818129 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e94f29dda34266b81b1c2f3790661e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/1000]  loss: -1.7837619042766164, policy_loss: -1.7912456436611581, kl-penalty: 7.485006778951301, entropy:45.73046875\n",
      "INFO 09-17 23:46:51 [executor_base.py:226] It took 0.289783 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:46:54 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da395528cb8e474b935c53e25c545e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:47:05 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:47:05 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:47:05 [worker.py:145] Sleep mode freed 15.32 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:47:05 [executor_base.py:210] It took 0.809687 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef7df68feb44d3981ab407492ceebb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/1000]  loss: -2.483104817369167, policy_loss: -2.491120774092031, kl-penalty: 8.018009195806812, entropy:44.857421875\n",
      "INFO 09-17 23:48:21 [executor_base.py:226] It took 0.289258 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:48:24 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8758b6ea08c74ebb9637e12314bea793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:48:37 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:48:37 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:48:38 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:48:38 [executor_base.py:210] It took 0.873916 seconds to fall asleep.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeed4250ae6495abd2cace4df04f362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/1000]  loss: -2.936158185832028, policy_loss: -2.9449980503019995, kl-penalty: 8.843798161375283, entropy:38.380859375\n",
      "INFO 09-17 23:50:04 [executor_base.py:226] It took 0.304779 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:50:07 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b590455ca2433789ca5475a1282f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:50:19 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:50:19 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:50:20 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:50:20 [executor_base.py:210] It took 0.830041 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50477627de9b49f085326b95fdb2c0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/1000]  loss: -1.8961768674344057, policy_loss: -1.9064730863410493, kl-penalty: 10.297069373697035, entropy:36.14453125\n",
      "INFO 09-17 23:51:46 [executor_base.py:226] It took 0.315109 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:51:48 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07598dda1d548faa9fe226b0effaa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:52:00 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:52:00 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:52:01 [worker.py:145] Sleep mode freed 15.28 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:52:01 [executor_base.py:210] It took 0.836470 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f3792593024312873dc8d5b31a0f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/1000]  loss: -0.7127670500121894, policy_loss: -0.72274051151557, kl-penalty: 9.970534156652812, entropy:34.197265625\n",
      "INFO 09-17 23:53:27 [executor_base.py:226] It took 0.310547 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:53:30 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d312dae507d1437fac20e204b8c028b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-17 23:53:43 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:53:43 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
      "INFO 09-17 23:53:44 [worker.py:145] Sleep mode freed 15.32 GiB memory, 41.24 GiB memory is still in use.\n",
      "INFO 09-17 23:53:44 [executor_base.py:210] It took 0.886988 seconds to fall asleep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54a9257b5b2494799ba6c654e385ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/1000]  loss: -0.5930978904943913, policy_loss: -0.6028316898900647, kl-penalty: 9.732563124642063, entropy:34.84765625\n",
      "INFO 09-17 23:55:09 [executor_base.py:226] It took 0.278431 seconds to wake up tags {'kv_cache', 'weights'}.\n",
      "WARNING 09-17 23:55:12 [executor_base.py:215] Executor is not sleeping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c60394f32ec4edc938d3fa5af129603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AgentConfig()\n",
    "agent = Agent(config, tokenizer, DEVICE)\n",
    "agent.update_inference_engine()\n",
    "hist = agent.train(dataset[\"train\"], dataset[\"test\"], 1000, 0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1fbd1b1ba52d44f2b58db62711c8ea01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a81584af09284075823fd2a697ff7bd9",
      "placeholder": "",
      "style": "IPY_MODEL_f5929d5929f8452eac604e5f29b4c78b",
      "value": "Addingrequests:100%"
     }
    },
    "2323b8aff3754638bb89a1b4194eb2a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "322e479f3c064b9cb3f4eecbececfadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "395e0505469c4116a3c4deecdd0402ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a9c6ffa426b40778f3a4c066fdb2c66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "453d0466c79a46868218d865965306e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b6cbc607ca24c4899c3e3eed4feabb6",
      "placeholder": "",
      "style": "IPY_MODEL_2323b8aff3754638bb89a1b4194eb2a1",
      "value": "Processedprompts:100%"
     }
    },
    "4a3c98d755964e89a4a64540dc27b805": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "51b1299a69514c369a9138e5cf7983a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b6cbc607ca24c4899c3e3eed4feabb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64d0a1f7561740328bfe8661ea99e589": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_395e0505469c4116a3c4deecdd0402ae",
      "placeholder": "",
      "style": "IPY_MODEL_4a3c98d755964e89a4a64540dc27b805",
      "value": "8/8[00:06&lt;00:00,1.73s/it,est.speedinput:159.57toks/s,output:810.73toks/s]"
     }
    },
    "7400ad1f10364e9b9873b2542ed5fda6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51b1299a69514c369a9138e5cf7983a7",
      "placeholder": "",
      "style": "IPY_MODEL_3a9c6ffa426b40778f3a4c066fdb2c66",
      "value": "2/2[00:00&lt;00:00,206.77it/s]"
     }
    },
    "7fdfc7ab7a134e6f96eced787204f2a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "813cf04bcdce4b50b57b6789f76e0f24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_453d0466c79a46868218d865965306e3",
       "IPY_MODEL_fb5646129fa248c1a70c9e1f6e5486ae",
       "IPY_MODEL_64d0a1f7561740328bfe8661ea99e589"
      ],
      "layout": "IPY_MODEL_ef23b970b1cf4f93914609cd3f759386"
     }
    },
    "87338622bacb4aac892a5d69fad37c31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9973e3cdd0a148eb93d24351bb68e4a5",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b939e2160fe1421d8fde528f1a7a5f56",
      "value": 2
     }
    },
    "9973e3cdd0a148eb93d24351bb68e4a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a81584af09284075823fd2a697ff7bd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b939e2160fe1421d8fde528f1a7a5f56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdb3910695af49869e149c96d3446b56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1fbd1b1ba52d44f2b58db62711c8ea01",
       "IPY_MODEL_87338622bacb4aac892a5d69fad37c31",
       "IPY_MODEL_7400ad1f10364e9b9873b2542ed5fda6"
      ],
      "layout": "IPY_MODEL_7fdfc7ab7a134e6f96eced787204f2a5"
     }
    },
    "ef23b970b1cf4f93914609cd3f759386": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "f45acffdb48e419591c72273d4d5310f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5929d5929f8452eac604e5f29b4c78b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb5646129fa248c1a70c9e1f6e5486ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f45acffdb48e419591c72273d4d5310f",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_322e479f3c064b9cb3f4eecbececfadb",
      "value": 8
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
